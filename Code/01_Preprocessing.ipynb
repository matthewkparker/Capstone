{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- This notebook will walk through the steps of taking the raw text files provided by kaggle, formatting it to remove irrelevant data, and then changing the structure of the data so that it can be used in a recommender system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from collections import deque\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data is provided in format that is difficult to work with, we have to format it so that we are able to both visualize it better, and work with it easier. This function goes through the following steps to do this\n",
    "- Step 1: This step reads the text files given a specific path, and assigns the columns names\n",
    "- Step 2: This step find the rows in the data frame where the rating column is missing. This is done because where the rows where the review is missing, there is the movie_id number. A list is created that contains the index of where the movie_id is, as well the movie_id itself.\n",
    "- Step 3: This step shift the entire the entire list of movie_id's and their indexes down by one.\n",
    "- Step 4: This step looks at the original list of movie_id's and their indexes, the shifted list of movie_id's and their indexes, makes sure that each movie_id is assigned to a copy of the data frame as long as they are between the two movie_id indexes, and then creates a new column of the actual movie id itself. The end is a single data frame for each movie of all the reviews for each movie, and the movie_id being a new column\n",
    "- Step 5: This final step then takes the list of the data frames created in step 4, combines them all together, print that the formatting is done, and returns the final, formatted data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting(path):\n",
    "    #Step 1\n",
    "    df_raw = pd.read_csv(path, header=None, names=['user_id', 'rating', 'date'], usecols=[0, 1, 2])\n",
    "    #Step 2\n",
    "    tmp_movies = df_raw[df_raw['rating'].isna()]['user_id'].reset_index()\n",
    "    movie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n",
    "    #Step 3\n",
    "    shifted_movie_indices = deque(movie_indices)\n",
    "    shifted_movie_indices.rotate(-1)\n",
    "    #Step 4\n",
    "    user_data = []\n",
    "    for [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n",
    "        if df_id_1<df_id_2:\n",
    "            tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n",
    "        else:\n",
    "            tmp_df = df_raw.loc[df_id_1+1:].copy()\n",
    "        tmp_df['movie'] = movie_id\n",
    "        user_data.append(tmp_df)\n",
    "    #Step 5\n",
    "    df = pd.concat(user_data)\n",
    "    print('done formatting')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data provided is split into 4 separate text files, so the function created above will be run for all 4 text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done formatting\n",
      "done formatting\n",
      "done formatting\n",
      "done formatting\n"
     ]
    }
   ],
   "source": [
    "df1 = formatting('../Data/combined_data_1.txt')\n",
    "df2 = formatting('../Data/combined_data_2.txt')\n",
    "df3 = formatting('../Data/combined_data_3.txt')\n",
    "df4 = formatting('../Data/combined_data_4.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because our data set is so large, we have to filter it to only contain users and movies that have a significant number of reviews. The goal is to only recommend movies that have proven their quality with many reviews, and to user reviews who have clear preferences of movies to recommend with. This function does this by grouping for every variable for a column, and then determining how many instances each individual variable has, and only returns that ones that have more instances than the provided threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_threshold(threshold,column,df):\n",
    "    filtered_df = df[df.groupby(column)[column].transform('size') >= threshold]\n",
    "    print('This function dropped',len(df)-len(filtered_df),'rows by filtering on the',column,'column')\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This for loop applies the function created above to each of of the 4 data frames. \n",
    "- Step 1: The for loop first drops the Date column, as it will not be used in recommending reviews to others. \n",
    "- Step 2: This step applies the function above to each data frame, and drops users that have less than 150 reviews, and drops movies that have less than 15,000 reviews\n",
    "After the for loop finishes, It is important to delete the old data frames from memory. This is done because the files that are being processed are somewhat large, and this step helps preserve RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Dataframe 1 :\n",
      "This function dropped 14870913 rows by filtering on the user_id column\n",
      "This function dropped 5612461 rows by filtering on the movie column\n",
      "For Dataframe 2 :\n",
      "This function dropped 15290681 rows by filtering on the user_id column\n",
      "This function dropped 6251804 rows by filtering on the movie column\n",
      "For Dataframe 3 :\n",
      "This function dropped 14808080 rows by filtering on the user_id column\n",
      "This function dropped 5096893 rows by filtering on the movie column\n",
      "For Dataframe 4 :\n",
      "This function dropped 15757559 rows by filtering on the user_id column\n",
      "This function dropped 5627403 rows by filtering on the movie column\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "i = 1\n",
    "for df in [df1,df2,df3,df4]:\n",
    "    print('For Dataframe',i,':')\n",
    "    #Step 1\n",
    "    df.drop(columns=['date'],inplace = True)\n",
    "    #Step 2\n",
    "    dropped_user = drop_threshold(150,'user_id',df)\n",
    "    df_list.append(drop_threshold(15000,'movie',dropped_user))\n",
    "    i += 1\n",
    "del df1\n",
    "del df2\n",
    "del df3\n",
    "del df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This cell combines all 4 of the filtered data frames into one large data frames, and assigns the proper data type to the user_id column. This cell also delete the list of the filtered data frames in order to preserve RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.concat(df_list)\n",
    "user_df['user_id'] = pd.to_numeric(user_df['user_id'])\n",
    "del df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This cell converts the provided movie csv, which consists of the movie title, which year it was released, and its corresponding movie id value, into a pandas data frame. It then creates a dictionary linking all of the movie titles to their respective movie id's, and then replacing the movie ids from the review data frame with the movie title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"../Data/movie_titles.csv\",encoding = 'latin1')\n",
    "movie_dict = dict(zip(movies['movie_id'],movies['title']))\n",
    "del movies\n",
    "user_df['movie'] = user_df['movie'].map(movie_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This cell pivots our review data frame so that each user is their own row, each movie is its own column, and the values being each individuals score for the movie is they did review it. If a user did note review a particular user, than that value will be indicated with a NA, or null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = user_df.pivot_table(values='rating',index='user_id',columns='movie')\n",
    "del user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This cell removes each users bias from the reviews. What this means in context is that for some users, they will rate every movie they enjoy as a 4, and the ones they really enjoy as a 5, while some will rate movies they enjoy as a 3, and the ones they really enjoy as a 4 or 5. By subtracting the average review from each review, we are able to the if an individual user enjoyed a particular movie more or less than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(data,axis = 1)\n",
    "for row in data.index:\n",
    "    data.loc[row] = data.loc[row] - means[row]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This cell repeats the process above, but for movies. For example, a movie like star wars may have a lot of 5 star reviews. If an individual person were to rate it 5 stars, that doesn't mean a lot. If a user were to rate a movie higher than the average rating for the movie, that would indicate that particular movie was especially appealing to that user when compared to others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_means = np.mean(data,axis = 0)\n",
    "for movie in data.columns:\n",
    "    data[movie] = data[movie] - col_means[movie]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By filling the missing reviews with 0, we imply that there are no positive or negative association with that particular movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0,inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This function's purpose is to convert a pandas data frame into a csv file, and then copying that file into a SQL database. This function is much more efficient than the default pandas function ```df.to_sql```. This function was created by stack overflow user mgoldwasser. The post of this code can be found at https://stackoverflow.com/questions/31997859/bulk-insert-a-pandas-dataframe-using-sqlalchemy/33529549"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_table(df, db_engine, schema, table_name, if_exists='replace'):\n",
    "    data.reset_index(inplace = True)\n",
    "    string_data_io = io.StringIO()\n",
    "    df.to_csv(string_data_io, sep='|', index=False)\n",
    "    pd_sql_engine = pd.io.sql.pandasSQL_builder(db_engine, schema=schema)\n",
    "    table = pd.io.sql.SQLTable(table_name, pd_sql_engine, frame=df,\n",
    "                               index=False, if_exists=if_exists, schema=schema)\n",
    "    table.create()\n",
    "    string_data_io.seek(0)\n",
    "    string_data_io.readline()\n",
    "    with db_engine.connect() as connection:\n",
    "        with connection.connection.cursor() as cursor:\n",
    "            copy_cmd = \"COPY %s.%s FROM STDIN HEADER DELIMITER '|' CSV\" % (schema, table_name)\n",
    "            cursor.copy_expert(copy_cmd, string_data_io)\n",
    "        connection.connection.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This cell is creating the engine, or the connection, with the postgreSQL server created on an AWS instance\n",
    "- Note: The password and IP_address are not provided in this notebook. If you want to run this locally, I recommend that you save the ```data``` data frame to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgres://postgres:password@ip_address:5432/postgres')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This final cell applies the function above, and defines the table name as raw_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_table(data,engine,'public','raw_user')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
